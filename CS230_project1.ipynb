{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from Kaggle (simple LSTM for text classification)\n",
    "#https://www.kaggle.com/kredy10/simple-lstm-for-text-classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Flatten, Input, Embedding, Bidirectional, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv(\"small_fic_dataset.tsv\",delimiter='\\t',encoding='latin-1')\n",
    "df.head()\n",
    "df.info()\n",
    "sns.countplot(df.MAINTAG)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of fics in each rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get input and label vectors\n",
    "X = df.TEXT\n",
    "print(X.shape)\n",
    "Y = df.MAINTAG\n",
    "print(X.shape)\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and test sets\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "Y_train = to_categorical(Y_train, num_classes=4)\n",
    "Y_test = to_categorical(Y_test, num_classes=4)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "max_len = 220\n",
    "tok = Tokenizer(lower=True) # default lower case = true\n",
    "tok.fit_on_texts(X_train)  # \n",
    "max_words = len(tok.word_index.items()) + 1 # vocal size, all words from all these stories? seems very small\n",
    "print(max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get glove embeddings\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "embedding_matrix = np.zeros((max_words,100))\n",
    "\n",
    "for word,i in tok.word_index.items():\n",
    "    print(word)\n",
    "    print(i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes padded sequences so all inputs are of the same length\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a simple CNN with max pooling\n",
    "def CNN():\n",
    " \n",
    "    inputs = Input(name='inputs',shape=[max_len], dtype='int32') # do we need dtype = 'int32'?\n",
    "    embedded_sequences = Embedding(max_words,100,weights=[embedding_matrix],input_length=max_len)(inputs)\n",
    "    layer = Conv1D(32, 5, activation='relu')(embedded_sequences)\n",
    "    layer = MaxPooling1D(5)(layer)\n",
    "    #layer = Conv1D(32, 5, activation='relu')(layer)\n",
    "    #layer = MaxPooling1D(5)(layer)\n",
    "    layer = Conv1D(32, 5, activation='relu')(layer)\n",
    "    layer = MaxPooling1D(1)(layer)  # global max pooling\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(32, activation='relu')(layer)\n",
    "    preds = Dense(4, activation='softmax')(layer)\n",
    "    \n",
    "    model = Model(inputs, preds)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len]) # main input _ story texts\n",
    "    # use glove here\n",
    "    layer = Embedding(max_words,100,weights=[embedding_matrix],input_length=max_len)(inputs)\n",
    "    layer = Bidirectional(LSTM(64))(layer)\n",
    "    \n",
    "    # lstm_out = \n",
    "    # add other input? auxiliary_input = Input(shape=(..,), name='aux_input')\n",
    "    # x = keras.layers.concatenate([lstm_out, auxiliary_input])\n",
    "    \n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(4,name='out_layer')(layer)\n",
    "   \n",
    "    layer = Activation('softmax')(layer) # add other inputs before this \n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    \n",
    "    # model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run RNN model with LSTM\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#trains model\n",
    "model.fit(sequences_matrix,Y_train,epochs=6,validation_split=0.3,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on test set and print accuracy\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "\n",
    "#plot_confusion_matrix(Y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
